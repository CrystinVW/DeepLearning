'''This programs deals with logistic regression, gradient descent, min-max scaling,
standardization, and plotting'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# 1	Archive (zip) of the source codes. Please do not submit data.
# 2	A README file where you mention what programming language you used, Operating system name/version, Computer architecture.
# 3	A PDF/DOC/DOCX/ODT file with your answers to the following problems.

############# 1 ##########################
# Load the data into memory. Then, convert each of the categorical variables into numerical.
# For example, the 6th column ("housing") is a categorical variable having two values {"no","yes"}.
# We can replace "no" with number 0, and "yes" with number 1 in the entire 6th column.

dataset_train = pd.read_csv("/Users/crystinrodrick/PycharmProjects/MachineLearning2018/bank-small-train.csv", sep=";")
dataset_test = pd.read_csv("/Users/crystinrodrick/PycharmProjects/MachineLearning2018/bank-small-test.csv", sep=";")
#print(dataset_train.shape)
#print(dataset)


# Transform the columns into numerical values
# Start with job
le = LabelEncoder()
dataset_train["job"] = le.fit_transform(dataset_train["job"])
dataset_test["job"] = le.fit_transform(dataset_test["job"])
#print(dataset)

#print(le.classes_)

le.fit_transform(le.classes_)

# Now change the marital column
dataset_train["marital"] = le.fit_transform(dataset_train["marital"])
dataset_test["marital"] = le.fit_transform(dataset_test["marital"])

# Transform education column
dataset_train["education"] = le.fit_transform(dataset_train["education"])
dataset_test["education"] = le.fit_transform(dataset_test["education"])

# Transform default column
dataset_train["default"] = le.fit_transform(dataset_train["default"])
dataset_test["default"] = le.fit_transform(dataset_test["default"])

# Transform housing column
dataset_train["housing"] = le.fit_transform(dataset_train["housing"])
dataset_test["housing"] = le.fit_transform(dataset_test["housing"])
#print(dataset)

# Transform loan column
dataset_train["loan"] = le.fit_transform(dataset_train["loan"])
dataset_test["loan"] = le.fit_transform(dataset_test["loan"])

# Transform contact column
dataset_train["contact"] = le.fit_transform(dataset_train["contact"])
dataset_test["contact"] = le.fit_transform(dataset_test["contact"])

# Transform month column
dataset_train["month"] = le.fit_transform(dataset_train["month"])
dataset_test["month"] = le.fit_transform(dataset_test["month"])

# Transform poutcome column
dataset_train["poutcome"] = le.fit_transform(dataset_train["poutcome"])
dataset_test["poutcome"] = le.fit_transform(dataset_test["poutcome"])

# Transform y column
dataset_train["y"] = le.fit_transform(dataset_train["y"])
dataset_test["y"] = le.fit_transform(dataset_test["y"])

#print(dataset_train["y"].value_counts())

# The independent variables in the dataset
X = dataset_train.iloc[:, :-1].values

# The dependent variable y
y = dataset_train.iloc[:, -1].values

# The independent variables in the dataset
X_test = dataset_test.iloc[:, :-1].values

# The dependent variable y
y_test = dataset_test.iloc[:, -1].values

# Add a one column in X_test
X_test = np.c_[np.ones((X_test.shape[0])),X_test]


averages = []


# performance measures
accuracy_list = []
accuracy_avg = []
precision_list = []
precision_avg = []
recall_list = []
recall_avg = []
f1_score_list = []
f1_avg = []


# Accuracy = (true positive + true negative) / (true positive + true negative + false positive + false negative)
def accuracy(matrix_stats):
    try:
        ac = ( (matrix_stats[0] + matrix_stats[1]) / (matrix_stats[0] + matrix_stats[1] +
                                                                      matrix_stats[2] + matrix_stats[3]))
    except ZeroDivisionError:
        ac = 0
    print("Accuracy: ", ac)
    accuracy_list.append(ac)
    accuracy_avg.append(ac)
    return ac

# Precision = True positive / (True positive + False Positive)
def precision(matrix_stats):
    try:
        precision = matrix_stats[0] / (matrix_stats[0] + matrix_stats[2])
    except ZeroDivisionError:
        precision = 0

    precision_list.append(precision)
    precision_avg.append(precision)
    print("Precision: ", precision)
    return precision

# Recall = True Positive / (True Positive + False Negative)
def recall(matrix_stats):
    try:
        recall = matrix_stats[0] / (matrix_stats[0] + matrix_stats[3])
    except ZeroDivisionError:
        recall = 0
    recall_list.append(recall)
    recall_avg.append(recall)
    print("Recall: ", recall)
    return recall

# F1-score = 2 * ((Precision * Recall) / (Precision + Recall))
def f1_score():
    try:
        score = 2 * ((precision_list[-1] * recall_list[-1]) / (precision_list[-1] + recall_list[-1]))
    except ZeroDivisionError:
        score = 0
    f1_score_list.append(score)
    f1_avg.append(score)
    print("F1 score: ", score)
    return score

############ 8 ################
# Summarize (using a plot, or a table) the classification performance metrics
# (i.e., accuracy, recall, precision, F1-score) you would obtain in each of the experiments above.
def plot_it(title):
    # x-coordinates of left sides of bars
    left = [0, 0.33, 0.66, 1]

    # heights of bars
    height = [accuracy_list[-1], precision_list[-1], recall_list[-1], f1_score_list[-1]]

    # labels for bars
    tick_label = ['Accuracy', "Precision", 'Recall', 'F1_score']

    # plotting a bar chart
    plt.bar(left, height, tick_label=tick_label,
            width=0.2, color=['red', 'green', 'blue', 'yellow'])

    # naming the x-axis
    plt.xlabel('Performance Matrix')
    # naming the y-axis
    plt.ylabel('Percentage %')

    # plot title
    plt.title(title)

    # function to show the plot
    plt.show()


###### 2 #################
# Now, implement logistic regression with SSE as loss function the cost function from class 2/5/2018.
# You need to solve it using the batch "Gradient Descent" algorithm. For the convergence test, choose
# option 1 with different nEpoch parameters from {100, 500, 1000}.



# Add a one column in X
X = np.c_[np.ones((X.shape[0])),X]

# Initialize w
w = np.random.uniform(size=(X.shape[1],))

def sigmoidFunction(z):
    return 1.0 / (1.0 + np.exp(-z))

# Find the global minimum, since it will sample the entire dataset at every iteration
matrix = []
def gradient_descent(w, Xs, y, nEpoch, alpha, lambdA, title, fold):
    list_losses = []
    tp = []
    tn = []
    fp = []
    fn = []
    #if scaling == 'min':
        ## Add a one column in X
        #Xs = min_max_scaling(Xs)


    for epoch in np.arange(0, nEpoch):

        # When adding regularization ((λ / m) * || w || ^ 2)
        hypo = sigmoidFunction(Xs.dot(w))
        error = hypo - y
        loss = np.sum(error**2)
        gradient = Xs.T.dot(error)
        gradient = gradient + ((lambdA / nEpoch) * abs(w)**2)
        w = w - alpha * gradient

        #print(int(hypo[epoch]), int(y[epoch]))
        if hypo[epoch] == 1 and y[epoch] == 1:
            tp.append(1)
        if hypo[epoch] == 0 and y[epoch] == 0:
            tn.append(1)
        if hypo[epoch] == 1 and y[epoch] == 0:
            fp.append(1)
        if hypo[epoch] == 0 and y[epoch] == 1:
            fn.append(1)

        #print("epoch #{}, loss={:.7f}".format(epoch+1,loss))
        list_losses.append(loss)
    average_loss = sum(list_losses) / len(list_losses)
    averages.append(average_loss)

    stats = len(tp), len(tn), len(fp), len(fn)
    matrix.append(stats)
    accuracy(stats)
    precision(stats)
    recall(stats)
    f1_score()

    # Plot it
    if fold == 0:
        plot_it(title)

#  Number of iterations 100
nEpoch = 100
alpha = 0.01
gradient_descent(w, X, y, nEpoch, alpha, 0, "100 nEpoch", 0)
#print(matrix)
del matrix[:]

#  Number of iterations 500
nEpoch = 500
alpha = 0.01
gradient_descent(w, X, y, nEpoch, alpha, 0, '500 nEpoch', 0)
del matrix[:]

# With number of iterations at 1000
nEpoch = 1000
alpha = 0.01
gradient_descent(w, X, y, nEpoch, alpha, 0, '1000 nEpoch', 0)
del matrix[:]

############ 3	###################
# Split the data randomly into two equal parts, containing 50% of the samples which will be used for training,
# and a test set containing the remaining 50% of the samples. Perform a 10-fold cross-validation to classify the
# training dataset using logistic regression you developed in step 2. Please report accuracy, precision, recall,
# F1-score in each step of cross-validation and also report the average of these individual metrics.
# This is going to be the training performance.


# Splits the datasets into different sizes (without standardization)
# Pre split training and testing sets for assignment's sake.

# This will no longer be needed because the training and test sets are pre-split
def splitData():#trainingSet):

    # Assume n = columns and m = rows
    # X = dataset().iloc[:, :-1].values  # Matrix of columns and rows
    Xtrain = dataset_train.iloc[:, :-1].values  # Matrix of columns and rows
    Ytrain = dataset_train.iloc[:, -1].values  # vector of rows x 1
    #print(X.shape, y.shape)
    #Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=trainingSet, random_state=123456)

    Xtest = dataset_test.iloc[:, :-1].values  # Matrix of columns and rows
    Ytest = dataset_test.iloc[:, -1].values
    #print(Xtest.shape)
    return Xtrain, Xtest, Ytrain, Ytest
splitData()

# Perform a 10 fold cross validation
def cross_validation(scaling, Xs):
    # A more robust and well-accepted variant of CV:
    # Step 1: Randomize the order of the training data samples.
    # Step 2: Perform k-fold cross validation on the reordered dataset.
    # Repeat steps 1 and 2 k times, and report average cross-validation performance.
    # Finally, fit the model with the entire training dataset.

    # loop "many" times
    k_fold = 10


    # The independent variables in the dataset
    #X = dataset_train.iloc[:, :-1].values

    # The dependent variable y
    #y = dataset_train.iloc[:, -1].values

    # Add a one column in X
    #X = np.c_[np.ones((X.shape[0])), X]

    iter = int(0)
    alpha = 0.01
    nEpoch = 100
    print('\n')
    for i in range(k_fold):
        print("Stats from fold {}".format(i + 1))
        X_train, X_test, y_train, y_test = train_test_split(
            Xs, y, test_size=0.1, random_state=iter)
        if scaling == 'min':
            gradient_descent(w, X_train, y_train, nEpoch, alpha, 0, 'Min-Max Scaling', 1)
        if scaling == 'standard':
            gradient_descent(w, X_train, y_train, nEpoch, alpha, 0, 'Standardization', 1)
        else:
            gradient_descent(w, X_train, y_train, nEpoch, alpha, 0, 'Results', 1)

        # Grab the stats
        #print("\n")

        #accuracy(matrix[iter])
        #precision(matrix[iter])
        #recall(matrix[iter])
        #f1_score()

        print("\n")

        # Increment
        iter += 1

# Perform cross validation with no scaling
cross_validation('no', X)

# Print results of average accuracy, precision, recall, and F1-score
def print_info_step_3():
    print("Average accuracy: ", np.sum(np.array(accuracy_avg)) / len(accuracy_avg))
    print("Average precision: ", np.sum(np.array(precision_avg)) / len(precision_avg))
    print("Average recall: ", np.sum(np.array(recall_avg)) / len(recall_avg))
    print("Average F1-score: ", np.sum(np.array(f1_avg)) / len(f1_avg))

    # Clear performance measures
    del accuracy_avg[:]
    del precision_avg[:]
    del recall_avg[:]
    del f1_avg[:]

# Print stats from matrix
print_info_step_3()

###################3B#####################
#  Now perform logistic regression on the entire training set,
# and evaluate the model on the test set. Please report accuracy, precision, recall, F1-score. This is going
# to be the test performance. Repeat this step with 3 different learning rates,  α  =  { 0.01 , 0.1 , 1  }   , and report the
# training and test performances.


def logistic_reg_w_train_and_test(X, y):
    # Clear matrix stats
    del matrix[:]

    # Alpha = 0.01
    gradient_descent(w, X, y, 100, 0.01, 0, 'Alpha = 0.01', 0)

    print("\n")
    print("Performance Measures for Alpha = 0.01 on set: ")

    # Reset matrix
    del matrix[:]

    # Alpha = 0.1
    print("\n")
    print("Performance Measures for Alpha = 0.1 on set: ")
    gradient_descent(w, X, y, 1000, 0.1, 0, 'Alpha =  0.1', 0)


    # Reset matrix
    del matrix[:]

    # Now call for Alpha = 1
    print("\n")
    print("Performance Measures for Alpha = 1 on set: ")
    gradient_descent(w, X, y, 1000, 1, 0, 'Alpha = 1', 0)

    del matrix[:]

# Test measures
print("\n")
print("Results from testing set: ")
logistic_reg_w_train_and_test(X, y_test)


############# 4 ###############
# Scale the features of the dataset using Min-Max scaling to [0,1] range, and repeat step 3.
# Please do not scale the y feature. And also do not scale the added column for the bias term
# having all 1s (i.e., x0=1 column)
# The independent variables in the dataset
X = dataset_train.iloc[:, :-1].values

copy_w = w.copy()
w = list(w)
del w[:1]
w = np.array(w)

def min_max_scaling(x):
    x_temp = []
    normalized = []
    for i in x:

        ind = 0
        del x_temp[:]

        for _ in i:
            norm = (i[ind] - min(i)) / (max(i) - min(i))
            ind += 1
            x_temp.append(norm)
        normalized.append(x_temp)
    #print(np.array(normalized))
    return np.array(normalized)

print("\nMIN MAX SCALING\n")

# Repeat step 3A
gradient_descent(w, min_max_scaling(X), y, 100, 0.1, 0, "Min-Max Scaling", 0)
del matrix[:]
# Call for cross validation with scaling
cross_validation('min', min_max_scaling(X))

#Print up the averages of the matrix
print_info_step_3()

# Repeat Step 3B
# Training Measures
#print("\n")
#print("Results from training set: ")
#logistic_reg_w_train_and_test(min_max_scaling(X), y, 'min')

# Test measures
print("\n")
print("Results from testing set: ")
logistic_reg_w_train_and_test(min_max_scaling(X), y_test)

# Keep the y feature as is so only the first x columns
# Now call for step 3 to happen again by calling a function that computes it


########## 5 ##################
# 	Scale the features of the dataset using standardization, and repeat step 3. Please do not
# scale the y feature. And also do not scale the added column for the bias term having all 1s
# (i.e., x0=1 column)

# Now do this with standardization
# Assume n = columns and m = rows

# Reestablish the X,y, X_test, Y_test
# The independent variables in the dataset
X = dataset_train.iloc[:, :-1].values

# The dependent variable y
y = dataset_train.iloc[:, -1].values



# The dependent variable y
y_test = dataset_test.iloc[:, -1].values

# Add a one column in X_test
X = np.c_[np.ones((X.shape[0])),X]

# Return the w to former glory
copy_w = list(copy_w)
w = np.array(copy_w)


# Standardization
def standardization(X, data):
    # Standardization
    scA = StandardScaler()
    if data == 'train':
        X = scA.fit_transform(X)
    if data == 'test':
        X = scA.fit_transform(X)
    else:
        pass
    return X

print("\nSTANDARDIZATION\n")

# Repeat step 3A
gradient_descent(w, X, y, 100, 0.1, 0, "Standardization", 0)
del matrix[:]
# Call for cross validation with scaling
cross_validation('standardization', standardization(X, 'train'))

#Print up the averages of the matrix
print("Averages from the k folds cross validation with standardization: ")
print_info_step_3()


# Repeat Step 3B

# Test measures
print("\n")
print("Results from testing set: ")
logistic_reg_w_train_and_test(standardization(X, 'train'), y_test)




############ 6 ################
# 	Implement regularized logistic regression with SSE as loss function the cost function from
# class 2/5/2018. Again, solve using the batch gradient descent algorithm. For the convergence test,
# choose option 1 with different nEpoch parameters from {100, 500, 1000}.

#                  m
# This is J(w) = - Σ [y sub i (log(h sub w(x)) + (1 - y sub i) * log(1- h sub w (h))] + ((λ/m) * ||w||^2)
#                 i=1

print("\nREGULARIZED LOGISTIC REGRESSION\n")

# Implement regularized logistic regression by changing the parameter to 1 instead of 0.
# RUN with different nEpoch

# With lambda 1
print('\nWITH EPOCH 100\n')
gradient_descent(w, X, y, 100, 0.1, 1, "With regularized log regression Epoch = 100", 0)
del matrix[:]


print('\nWITH EPOCH 500\n')
gradient_descent(w, X, y, 500, 0.1, 1, "With epoch = 500, reg. log regression", 0)
del matrix[:]


print('\nWITH EPOCH 1000\n')
gradient_descent(w, X, y, 1000, 0.1, 1, "With epoch = 1000, Reg. Log Regression", 0)
del matrix[:]



############ 7 ################
# 	On the standardized dataset, repeat step 3 except using the regularized logistic regression
# you developed in step 6, by varying the parameter, λ={0,1,10,100,1000}.

print('\nSTANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION\n')
print("\nWITH LAMBDA = 0")
gradient_descent(w, X, y, 1000, 0.1, 0, 'STANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION'
                                                             'Lambda = 0', 0)
del matrix[:]
# Step 3A
# Call for cross validation with scaling
cross_validation('standardization', X)

#Print up the averages of the matrix
print("Averages from the k folds cross validation with regularization off of standardization: ")
print_info_step_3()


# Step 3B

# Test measures
print("\n")
print("Results from testing set: ")
logistic_reg_w_train_and_test(X, y_test)


# NOW DO THIS WITH LAMBDA = 1
print('\nSTANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION\n')
print("\nWITH LAMBDA = 1")
gradient_descent(w, X, y, 1000, 0.1, 1, 'STANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION'
                                                             'Lambda = 1', 0)
del matrix[:]
# Step 3A
# Call for cross validation with scaling
cross_validation('standardization', X)

#Print up the averages of the matrix
print("Averages from the k folds cross validation with regularization off of standardization: ")
print_info_step_3()


# Step 3B

# Test measures
print("\n")
print("Results from testing set: ")
logistic_reg_w_train_and_test(X, y_test)


# NOW DO THIS WITH LAMBDA = 10
print('\nSTANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION\n')
print("\nWITH LAMBDA = 10")
gradient_descent(w, X, y, 1000, 0.1, 10, 'STANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION'
                                                             'Lambda = 10', 0)
del matrix[:]
# Step 3A
# Call for cross validation with scaling
cross_validation('standardization', X)

#Print up the averages of the matrix
print("Averages from the k folds cross validation with regularization off of standardization: ")
print_info_step_3()


# Test measures
print("\n")
print("Results from testing set: ")
logistic_reg_w_train_and_test(X, y_test)


# NOW DO THIS WITH LAMBDA = 100
print('\nSTANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION\n')
print("\nWITH LAMBDA = 100")
gradient_descent(w, X, y, 1000, 0.1, 100, 'STANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION'
                                                             'Lambda = 100')

# Call for cross validation with scaling
# Step 3A
cross_validation('standardization')

#Print up the averages of the matrix
print("Averages from the k folds cross validation with regularization off of standardization: ")
print_info_step_3()


# Step 3B
# Training Measures
print("\n")
print("Results from training set: ")
logistic_reg_w_train_and_test(X, y, 'standard')

# Test measures
print("\n")
print("Results from testing set: ")
logistic_reg_w_train_and_test(X, y_test, 'standard')


# NOW DO THIS WITH LAMBDA = 1000
print('\nSTANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION\n')
print("\nWITH LAMBDA = 1000")
gradient_descent(w, X, y, 1000, 0.1, 1000, 'STANDARDIZED DATASET WITH REGULARIZED LOGISTIC REGRESSION'
                                                             'Lambda = 1000')
#Step 3A
# Call for cross validation with scaling
cross_validation('standardization')

#Print up the averages of the matrix
print("Averages from the k folds cross validation with regularization off of standardization: ")
print_info_step_3()


# Step 3B
# Training Measures
print("\n")
print("Results from training set: ")
logistic_reg_w_train_and_test(X, y, 'standard')

# Test measures
print("\n")
print("Results from testing set: ")
logistic_reg_w_train_and_test(X, y_test, 'standard')
